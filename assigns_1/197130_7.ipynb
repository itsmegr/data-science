{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NESpmRSxdmC"
      },
      "source": [
        "Q1> Implement a Simple Neural Network Model and predict if a person will buy insurance or not\n",
        "for the given dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Name : Govind Rathore\n",
        "Roll no : 197130\n",
        "Sec : A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "Q0VV2UYyCXsf",
        "outputId": "0bf3927a-76a5-45e5-fc04-84606460e107"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/itsmegr/Desktop/Codehub/data-science/197130_7.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/itsmegr/Desktop/Codehub/data-science/197130_7.ipynb#ch0000002?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m files\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/itsmegr/Desktop/Codehub/data-science/197130_7.ipynb#ch0000002?line=1'>2</a>\u001b[0m uploaded\u001b[39m=\u001b[39mfiles\u001b[39m.\u001b[39mupload()\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2HUYpfkCHDW",
        "outputId": "79890684-c35c-4ddd-d6cf-4fb88b4828be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   age  Affordability  bought_insurance\n",
            "0   22              1                 0\n",
            "1   25              1                 0\n",
            "2   47              0                 1\n",
            "3   52              1                 0\n",
            "4   46              1                 1\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 0.7958 - accuracy: 0.5000\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7944 - accuracy: 0.5000\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7931 - accuracy: 0.5000\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7917 - accuracy: 0.5000\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7904 - accuracy: 0.5000\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7891 - accuracy: 0.5000\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7878 - accuracy: 0.5000\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7865 - accuracy: 0.5000\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7853 - accuracy: 0.5000\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7840 - accuracy: 0.5000\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7828 - accuracy: 0.5000\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7816 - accuracy: 0.5000\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7804 - accuracy: 0.5000\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7792 - accuracy: 0.5000\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7780 - accuracy: 0.5000\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7768 - accuracy: 0.5000\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7756 - accuracy: 0.5000\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7744 - accuracy: 0.5000\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7733 - accuracy: 0.5000\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7721 - accuracy: 0.5000\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7709 - accuracy: 0.5000\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7698 - accuracy: 0.5000\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7686 - accuracy: 0.5000\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7675 - accuracy: 0.5000\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7664 - accuracy: 0.5000\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7654 - accuracy: 0.5000\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7643 - accuracy: 0.5000\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7632 - accuracy: 0.5000\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7622 - accuracy: 0.5000\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7611 - accuracy: 0.5000\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7601 - accuracy: 0.5000\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7591 - accuracy: 0.5000\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7580 - accuracy: 0.5000\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7570 - accuracy: 0.5000\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7560 - accuracy: 0.5000\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7551 - accuracy: 0.5000\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7541 - accuracy: 0.5000\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7531 - accuracy: 0.5000\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7522 - accuracy: 0.5000\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7512 - accuracy: 0.5000\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7503 - accuracy: 0.5000\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7494 - accuracy: 0.5000\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7484 - accuracy: 0.5000\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7475 - accuracy: 0.5000\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7466 - accuracy: 0.5000\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7457 - accuracy: 0.5000\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7448 - accuracy: 0.5000\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7439 - accuracy: 0.5000\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7430 - accuracy: 0.5000\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7421 - accuracy: 0.5000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.7430 - accuracy: 0.6667\n",
            "Accuracy is: 66.66666666666666\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.models import Sequential   \n",
        "from keras.layers import Dense        \n",
        "import pandas as pd                   \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt       \n",
        "import io                             \n",
        "from sklearn.preprocessing import StandardScaler     \n",
        "from sklearn.metrics import accuracy_score          \n",
        "\n",
        "## importing the dataset\n",
        "df= pd.read_csv('insurance_data.csv')           \n",
        "print(df.head())\n",
        "\n",
        "n=df.shape[1]               \n",
        "m=df.shape[0]               \n",
        "\n",
        "x_train=np.array(df.iloc[:int(0.9*m),:n-1])      \n",
        "y_train=np.array(df.iloc[:int(0.9*m),-1])        \n",
        "x_test=np.array(df.iloc[int(0.9*m):,:n-1])       \n",
        "y_test=np.array(df.iloc[int(0.9*m):,-1])         \n",
        "\n",
        "\n",
        "\n",
        "## here bought_insurance is already encoded as it can be a binary number\n",
        "sc = StandardScaler()\n",
        "x_train_scaled= sc.fit_transform(x_train)    # here we standardize the training and testing data using standardscaler()\n",
        "x_test_scaled=sc.fit_transform(x_test)       # we standardize them so that they are easy to compute\n",
        "\n",
        "model=Sequential()                                      # we create our neural network model here\n",
        "model.add(Dense(2, input_dim=2, activation='relu'))     # input dimensions to this layer is 2 and we use reLU activation function\n",
        "model.add(Dense(2, input_dim=2, activation='relu'))     # input dimensions to this layer is 2 and we use reLU activation function \n",
        "model.add(Dense(1, activation='softmax'))               # finally the output dimension has only 1 parameter(0 or 1) so we use softmax activation function  \n",
        "                                                        # output dimension has 1 parameter as output\n",
        "                                                       \n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_scaled, y_train, epochs=50,batch_size=25) # we fit our model using this\n",
        "model.evaluate(x_test_scaled,y_test)                        # we evaluate using our scaled testing data\n",
        "y_pred=model.predict(x_test_scaled)                         # we predict based on scaled testing data\n",
        "actual_res=y_test.tolist()                                  # actual_res contains the actual result list  \n",
        "pred_res=y_pred.tolist()                                    # pred_res contains the predicted result list\n",
        "\n",
        "\n",
        "a = accuracy_score(pred_res,actual_res)                     # we compute the accuracy based on accuracy_score function from sklearn.metrics\n",
        "print('Accuracy is:', a*100)                                # final accuracy result "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVrUDzDAxjyQ"
      },
      "source": [
        "Q2> Use the given dataset and build a customer churn prediction model using artificial neural network.\n",
        "[Concept used: ANN Classification, Use Multiple Hidden Layers, Confusion Matrix to check the\n",
        "accuracy rate of the model]\n",
        "Dataset : kaggle.com/datasets/mltuts/churn-modelling-data?select=Churn_Modelling.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "lVE56XW0yHw5",
        "outputId": "9517cc19-e3e2-4eb2-8b1a-9c6af148a2c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-75a17ea0-946b-43b1-acc0-4256551c4973\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-75a17ea0-946b-43b1-acc0-4256551c4973\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Churn_Modelling.csv to Churn_Modelling.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf9rFB8Oy6ce",
        "outputId": "005cf70f-1291-4526-8304-239d4b71fac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
            "0          1    15634602  Hargrave          619    France  Female   42   \n",
            "1          2    15647311      Hill          608     Spain  Female   41   \n",
            "2          3    15619304      Onio          502    France  Female   42   \n",
            "3          4    15701354      Boni          699    France  Female   39   \n",
            "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
            "\n",
            "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
            "0       2       0.00              1          1               1   \n",
            "1       1   83807.86              1          0               1   \n",
            "2       8  159660.80              3          1               0   \n",
            "3       1       0.00              2          0               0   \n",
            "4       2  125510.82              1          1               1   \n",
            "\n",
            "   EstimatedSalary  Exited  \n",
            "0        101348.88       1  \n",
            "1        112542.58       0  \n",
            "2        113931.57       1  \n",
            "3         93826.63       0  \n",
            "4         79084.10       0  \n",
            "The dataset after dopping first 3 columns are : \n",
            "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
            "0          619    France  Female   42       2       0.00              1   \n",
            "1          608     Spain  Female   41       1   83807.86              1   \n",
            "2          502    France  Female   42       8  159660.80              3   \n",
            "3          699    France  Female   39       1       0.00              2   \n",
            "4          850     Spain  Female   43       2  125510.82              1   \n",
            "\n",
            "   HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
            "0          1               1        101348.88       1  \n",
            "1          0               1        112542.58       0  \n",
            "2          1               0        113931.57       1  \n",
            "3          0               0         93826.63       0  \n",
            "4          1               1         79084.10       0  \n",
            "   CreditScore  Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
            "0          619          0       0   42       2       0.00              1   \n",
            "1          608          1       0   41       1   83807.86              1   \n",
            "2          502          0       0   42       8  159660.80              3   \n",
            "3          699          0       0   39       1       0.00              2   \n",
            "4          850          1       0   43       2  125510.82              1   \n",
            "\n",
            "   HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
            "0          1               1        101348.88       1  \n",
            "1          0               1        112542.58       0  \n",
            "2          1               0        113931.57       1  \n",
            "3          0               0         93826.63       0  \n",
            "4          1               1         79084.10       0  \n",
            "Epoch 1/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.7209 - accuracy: 0.4599\n",
            "Epoch 2/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.6105 - accuracy: 0.7226\n",
            "Epoch 3/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.5285 - accuracy: 0.7916\n",
            "Epoch 4/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.7944\n",
            "Epoch 5/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4636 - accuracy: 0.7949\n",
            "Epoch 6/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4528 - accuracy: 0.7959\n",
            "Epoch 7/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4458 - accuracy: 0.7968\n",
            "Epoch 8/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.7984\n",
            "Epoch 9/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4354 - accuracy: 0.8004\n",
            "Epoch 10/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4314 - accuracy: 0.8026\n",
            "Epoch 11/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8045\n",
            "Epoch 12/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4242 - accuracy: 0.8049\n",
            "Epoch 13/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4210 - accuracy: 0.8062\n",
            "Epoch 14/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4176 - accuracy: 0.8074\n",
            "Epoch 15/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4142 - accuracy: 0.8084\n",
            "Epoch 16/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4105 - accuracy: 0.8086\n",
            "Epoch 17/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4065 - accuracy: 0.8110\n",
            "Epoch 18/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4023 - accuracy: 0.8140\n",
            "Epoch 19/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.8170\n",
            "Epoch 20/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3931 - accuracy: 0.8219\n",
            "Epoch 21/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.8263\n",
            "Epoch 22/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3836 - accuracy: 0.8298\n",
            "Epoch 23/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.8324\n",
            "Epoch 24/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3755 - accuracy: 0.8345\n",
            "Epoch 25/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.8375\n",
            "Epoch 26/50\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3686 - accuracy: 0.8395\n",
            "Epoch 27/50\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3656 - accuracy: 0.8425\n",
            "Epoch 28/50\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3630 - accuracy: 0.8429\n",
            "Epoch 29/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3605 - accuracy: 0.8450\n",
            "Epoch 30/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3580 - accuracy: 0.8486\n",
            "Epoch 31/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3556 - accuracy: 0.8495\n",
            "Epoch 32/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3544 - accuracy: 0.8510\n",
            "Epoch 33/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3523 - accuracy: 0.8535\n",
            "Epoch 34/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3508 - accuracy: 0.8554\n",
            "Epoch 35/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3496 - accuracy: 0.8559\n",
            "Epoch 36/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3483 - accuracy: 0.8580\n",
            "Epoch 37/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3476 - accuracy: 0.8580\n",
            "Epoch 38/50\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3461 - accuracy: 0.8589\n",
            "Epoch 39/50\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3457 - accuracy: 0.8595\n",
            "Epoch 40/50\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3442 - accuracy: 0.8594\n",
            "Epoch 41/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3446 - accuracy: 0.8584\n",
            "Epoch 42/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3428 - accuracy: 0.8610\n",
            "Epoch 43/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3423 - accuracy: 0.8586\n",
            "Epoch 44/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3418 - accuracy: 0.8600\n",
            "Epoch 45/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3412 - accuracy: 0.8596\n",
            "Epoch 46/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3402 - accuracy: 0.8600\n",
            "Epoch 47/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3396 - accuracy: 0.8605\n",
            "Epoch 48/50\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.3394 - accuracy: 0.8614\n",
            "Epoch 49/50\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3386 - accuracy: 0.8599\n",
            "Epoch 50/50\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3381 - accuracy: 0.8606\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.3658 - accuracy: 0.8450\n",
            "Accuracy is: 84.5\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.models import Sequential   # For building the Neural Network layer by layer\n",
        "from keras.layers import Dense        # used to add fully connected layer in ANN#Simple Neural Network Model Code using Tensorflow\n",
        "import pandas as pd                   # import pandas and numpy for datafram and numpy operations\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt       # import matplotlib for graphical visualizations \n",
        "import io                             # import io for reading csv files\n",
        "from sklearn.preprocessing import StandardScaler      # for standardizing the data\n",
        "from sklearn.metrics import accuracy_score            # to determine the accuracy score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "## importing the dataset\n",
        "df= pd.read_csv('Churn_Modelling.csv')           \n",
        "print(df.head())\n",
        "\n",
        "print(\"The dataset after dopping first 3 columns are : \")\n",
        "df.drop(df.columns[[0,1,2]],axis=1,inplace=True)    # we drop few columns because we dont need them \n",
        "print(df.head())\n",
        "n=df.shape[1]               # n--> number of columns\n",
        "m=df.shape[0]               # m--> number of rows\n",
        "\n",
        "\n",
        "def texttonum(vec):\n",
        "  visited=[]\n",
        "  res=[]\n",
        "  index=0\n",
        "  for i in vec:\n",
        "    if(i in visited):\n",
        "      res.append(visited.index(i))\n",
        "    else:\n",
        "      visited.append(i)\n",
        "      res.append(index)\n",
        "      index=index+1\n",
        "  return res\n",
        "df.iloc[:,1]=np.array(texttonum(df.iloc[:,1].tolist()))        # converting the text data in geography and gender column into categorical data\n",
        "df.iloc[:,2]=np.array(texttonum(df.iloc[:,2].tolist()))        # we make 0 --> female 1--> male\n",
        "print(df.head())\n",
        "\n",
        "x_train=np.array(df.iloc[:int(0.8*m),:n-1])      # we take 80% of rows in training data\n",
        "y_train=np.array(df.iloc[:int(0.8*m),-1])        # only the last column is in y_train  \n",
        "x_test=np.array(df.iloc[int(0.8*m):,:n-1])       # only 20 % of rows is in testing dataset\n",
        "y_test=np.array(df.iloc[int(0.8*m):,-1])         # splitting into training and testing dataset\n",
        "\n",
        "\n",
        "\n",
        "sc = StandardScaler()\n",
        "x_train_scaled= sc.fit_transform(x_train)    # here we standardize the training and testing data using standardscaler()\n",
        "x_test_scaled=sc.fit_transform(x_test)       # we standardize them so that they are easy to compute\n",
        "\n",
        "\n",
        "model=Sequential()                                      # we create our neural network model here\n",
        "model.add(Dense(10, input_dim=10, activation='relu'))     # input dimensions to this layer is 2 and we use reLU activation function\n",
        "model.add(Dense(8, input_dim=10, activation='relu'))     # input dimensions to this layer is 2 and we use reLU activation function \n",
        "model.add(Dense(6, input_dim=8, activation='relu'))     # input dimensions to this layer is 2 and we use reLU activation function \n",
        "model.add(Dense(1, activation='sigmoid'))               # finally the output dimension has only 1 parameter(0 or 1) so we use sigmoid activation function  \n",
        "                                                        # output dimension has 1 parameter as output\n",
        "\n",
        "#\"compile\" is a method of Tensorflow. “adam’ is the optimizer that can perform the gradient descent.\n",
        "# loss function as binary_crossentropy as we have 2 cases only 0 or 1\n",
        "# The optimizer updates the weights during training and reduces the loss.                                                        \n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_scaled, y_train, epochs=50,batch_size=250) # we fit our model using this\n",
        "model.evaluate(x_test_scaled,y_test)                        # we evaluate using our scaled testing data\n",
        "y_pred=model.predict(x_test_scaled)                         # we predict based on scaled testing data\n",
        "actual_res=y_test.tolist()                                  # actual_res contains the actual result list  \n",
        "pred_res=y_pred.tolist()                                    # pred_res contains the predicted result list\n",
        "\n",
        "for i in range(len(pred_res)):                    # since we used sigmoid function as our last activation layer\n",
        "  if(pred_res[i][0]>=0.5):                        # we convert those values into either 0 or 1 based on whether they are greater than 0.5 or not\n",
        "    pred_res[i]=1                                 # if greater or equal to 0.5 we make it 1\n",
        "  else:                                            \n",
        "    pred_res[i]=0                                 # if less than 0.5 we make it equal to 0 \n",
        "\n",
        "a = accuracy_score(pred_res,actual_res)                     # we compute the accuracy based on accuracy_score function from sklearn.metrics\n",
        "print('Accuracy is:', a*100)                                # final accuracy result "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWAxyrZT_GJn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DataScienceLabAssignment7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
